---
title: 'S3 멀티파트 업로드의 원리: 바이트에서 영상까지'
summary: 대용량 파일을 분할하고 합치는 원리를 바이트 레벨에서 이해하기
date: '2025-12-23'
tags: ['AWS']
draft: false
---

대용량 파일을 S3에 업로드할 때 멀티파트 업로드를 사용하게 된다. 하지만 "파일을 쪼개서 업로드한다"는 개념이 실제로 어떻게 작동하는지, 그리고 왜 쪼개진 파일들이 다시 합쳐져서 완벽하게 작동할 수 있는지 깊이 이해하려면 파일의 본질부터 이해할 필요가 있다.

---

## 1. S3 멀티파트 업로드란?

S3 멀티파트 업로드는 큰 파일을 여러 개의 작은 파트로 나누어 업로드하는 방식이다. AWS에서는 100MB 이상의 파일에 대해서는 멀티파트 업로드 사용을 권장하고, 5GB 이상의 파일은 반드시 멀티파트 업로드를 사용해야 한다.

멀티파트 업로드는 세 단계로 진행된다. 먼저 **초기화(Initiate)** 단계에서 고유한 Upload ID를 받는다. 그다음 **파트 업로드(Upload Parts)** 단계에서 파일을 여러 파트(최소 5MB, 마지막 파트는 예외)로 나누어 각각 업로드한다. 각 파트는 1부터 10,000까지의 번호를 받으며, 업로드 완료 시 ETag를 받는다. 마지막으로 **완료(Complete)** 단계에서 Complete Multipart Upload 요청으로 파트들을 하나의 객체로 결합한다.

이 방식의 장점은 명확하다. 네트워크 문제로 업로드가 중단되어도 실패한 파트만 다시 업로드하면 되고, 여러 파트를 병렬로 업로드할 수 있어 대용량 파일의 업로드 속도를 크게 개선할 수 있다. 파일 크기를 미리 알 필요 없이 스트리밍 방식으로 업로드하는 것도 가능하다.

한 가지 주의할 점이 있다. 멀티파트 업로드를 시작했지만 완료하거나 중단하지 않으면 S3에 불완전한 파트들이 계속 저장되어 비용이 발생한다. 수명 주기 정책(Lifecycle Policy)으로 7일 이상 된 불완전한 멀티파트 업로드를 자동으로 삭제하도록 설정하는 것이 권장된다.

<MultipartProcessDemo />

---

## 2. Complete Multipart Upload의 동작 원리

Complete Multipart Upload는 S3에 "이제 모든 파트를 업로드했으니 이것들을 하나의 객체로 합쳐달라"고 요청하는 API 호출이다. 요청 본문은 XML 형태로, 각 파트의 번호와 ETag를 포함한다.

```xml
<CompleteMultipartUpload>
  <Part>
    <PartNumber>1</PartNumber>
    <ETag>"etag-value-1"</ETag>
  </Part>
  <Part>
    <PartNumber>2</PartNumber>
    <ETag>"etag-value-2"</ETag>
  </Part>
</CompleteMultipartUpload>
```

Complete 요청을 받으면 S3는 제공된 각 ETag가 실제로 업로드된 파트의 ETag와 일치하는지 검증하고, 파트 번호가 연속적이고 올바른 순서인지 확인한다. 그다음 파트들을 순서대로 연결하여 최종 객체를 생성하고, 최종 객체의 ETag를 계산한 뒤, 임시로 저장되어 있던 개별 파트 데이터를 정리한다.

멀티파트 업로드로 생성된 객체의 ETag는 일반 업로드와 다르다. 일반 업로드의 ETag는 파일 전체의 MD5 해시이지만, 멀티파트 객체의 ETag는 다음과 같은 형식이다.

```
"d41d8cd98f00b204e9800998ecf8427e-3"
```

마지막의 `-3`은 3개의 파트로 업로드되었다는 의미다. 실제 해시값은 각 파트의 MD5를 연결한 후 그것의 MD5를 계산한 것이다.

```
ETag = MD5(MD5(part1) + MD5(part2) + MD5(part3)) + "-" + partCount
```

S3는 분산 스토리지 시스템이기 때문에 각 파트는 다른 물리적 위치에 저장될 수 있다. Complete 요청이 들어오면 S3는 실제 데이터를 물리적으로 복사하는 것이 아니라, "이 객체는 파트 1, 2, 3으로 구성되어 있다"는 메타데이터를 생성한다. 이 방식 덕분에 수 GB의 파일도 Complete 요청은 몇 초 안에 완료된다.

<ETagVisualization />

---

## 3. 파일의 본질: 바이트의 연속

여기서 핵심적인 질문이 생긴다. 영상 파일을 예로 들면, 어떻게 쪼개져 있는 각 파트의 영상 파일이 합쳐져 하나의 파일이 될 수 있는 것일까? 이를 이해하려면 파일이 무엇인지부터 알아야 한다.

파일은 결국 **순서가 있는 바이트의 나열**이다. 영상 파일도 마찬가지다.

```
video.mp4 = [byte0, byte1, byte2, ..., byteN]
```

예를 들어 100MB 영상 파일이 있다면 총 104,857,600 bytes이고, 실제 내용은 `0x00 0x00 0x00 0x18 0x66 0x74 0x79 0x70 ...`과 같은 바이트들의 나열이다. 이것을 3개 파트로 나누면 Part 1은 bytes [0 ~ 34,952,533], Part 2는 bytes [34,952,534 ~ 69,905,066], Part 3은 bytes [69,905,067 ~ 104,857,599]가 된다.

MP4 파일은 "박스(box)" 또는 "아톰(atom)"이라는 구조로 이루어져 있다.

```
[ftyp box - 파일 타입]
[moov box - 메타데이터: 코덱, 해상도, 재생시간 등]
  ├─ [mvhd - 전체 정보]
  ├─ [trak - 비디오 트랙]
  └─ [trak - 오디오 트랙]
[mdat box - 실제 비디오/오디오 데이터]
  ├─ 프레임 1 데이터
  ├─ 프레임 2 데이터
  └─ ...
```

하지만 **멀티파트 업로드는 이런 구조를 전혀 신경 쓰지 않는다**. 단순히 바이트 배열을 분할할 뿐이다.

```python
file_bytes = read_entire_file()
part_size = len(file_bytes) // 3

part1 = file_bytes[0:part_size]
part2 = file_bytes[part_size:part_size*2]
part3 = file_bytes[part_size*2:]
```

영상 파일의 헤더 중간을 자르든, 특정 프레임 중간을 자르든 상관없다. 그냥 바이트를 기계적으로 나눈다. 멀티파트 Complete는 단순히 바이트를 순서대로 연결(concatenate)하기 때문에, 원본과 정확히 동일한 바이트 시퀀스가 복원된다.

터미널에서 직접 확인할 수 있다.

```bash
# 1. 테스트 영상 파일 3개 파트로 분할
split -n 3 video.mp4 part_

# 2. 다시 합치기
cat part_aa part_ab part_ac > reconstructed.mp4

# 3. 원본과 비교
md5sum video.mp4
md5sum reconstructed.mp4
# -> 두 해시값이 완전히 동일!
```

영상 플레이어는 `reconstructed.mp4`를 원본과 똑같이 재생한다. 바이트 레벨에서 완전히 동일하기 때문이다.

이런 단순한 방식이 가능한 이유는 **파일 포맷 자체가 자기 서술적**(self-describing)이기 때문이다. 영상 플레이어는 파일의 처음부터 순차적으로 읽기 시작해서 헤더를 파싱하고, "어디에 무엇이 있는지" 파악한 후 그 위치로 점프(seek)해서 데이터를 읽는다. 멀티파트로 나뉘어 업로드되었든, 한 번에 업로드되었든 **최종적으로 바이트가 올바른 순서로 나열되어 있으면** 문제없다.

만약 순서가 바뀌면 MP4 헤더가 중간에 있게 되어 플레이어가 파일 타입을 인식하지 못하고, 바이트 순서가 뒤죽박죽이라 재생이 불가능해진다. 그래서 Complete 요청에 **반드시 PartNumber를 함께 보내야** 하고, S3는 이 순서대로 정확히 연결한다.

<FileProvider>

<HexDumpExplorer />

---

## 4. 바이트가 영상이 되는 과정

이제 더 근본적인 질문으로 들어가보자. 바이트로 표현된 것들이 어떻게 영상이 될 수 있을까?

바이트는 그 자체로는 그냥 0~255 사이의 숫자일 뿐이다. 같은 바이트 시퀀스라도 **어떻게 해석하느냐**에 따라 전혀 다른 것이 된다. 예를 들어 `0x48 0x65 0x6C 0x6C 0x6F`라는 바이트 시퀀스는 ASCII 텍스트로 해석하면 "Hello"가 되고, 정수로 해석하면 310,939,249,775가 되고, RGB 픽셀로 해석하면 빨강=72, 초록=101, 파랑=108이 된다.

영상은 수많은 픽셀(점)들의 격자다. 1920x1080 해상도는 2,073,600개의 픽셀이고, 각 픽셀은 RGB 3개 값(빨강, 초록, 파랑)을 가진다. 가장 단순한 비압축 형태로 저장하면 전체 프레임 하나가 1920 × 1080 × 3 = 6,220,800 bytes, 약 6MB가 된다. 30fps 영상 1초는 180MB다. 실제로는 너무 크기 때문에 H.264 같은 코덱으로 압축한다.

압축은 공간적 중복과 시간적 중복을 제거하는 방식으로 이루어진다. 하늘의 파란색 픽셀 1000개가 모두 비슷하다면 "픽셀 (0,0)부터 (31,31)까지 색상 0x0080FF"라고 한 번만 기록하면 된다. 프레임 1에서 자동차가 (100, 200) 위치에 있고 프레임 2에서 (105, 200) 위치에 있다면, 두 프레임을 모두 저장하는 대신 "프레임 1 저장 + 프레임 2는 5픽셀 오른쪽 이동"이라고 기록한다.

영상 플레이어가 하는 일은 세 단계다. 먼저 **파싱**(Parsing)에서 파일을 읽고 메타데이터(코덱, 해상도, 프레임 위치 등)를 파악한다. 그다음 **디코딩**(Decoding)에서 압축된 프레임 데이터를 H.264 디코더로 압축 해제하여 원본 픽셀 데이터를 복원한다. 마지막으로 **렌더링**(Rendering)에서 픽셀 데이터를 GPU에 전송하여 화면에 그린다.

모니터의 각 픽셀은 실제로 빨강, 초록, 파랑 LED/LCD로 구성되어 있다. 바이트 값 255는 LED 최대 밝기, 0은 LED 꺼짐, 128은 절반 밝기가 된다. RGB(255, 0, 0)이면 빨강 LED만 최대 밝기로 켜지고 나머지는 꺼져서 눈에는 빨간색으로 보인다.

<ByteInterpretationDemo />

---

## 5. 계층적 추상화: 전자에서 영상까지

전체 과정을 정리하면 다음과 같은 계층 구조가 된다.

```
물리 레벨:
전자의 흐름 → 전압 → LED 밝기

하드웨어 레벨:
0과 1 → 바이트 (8비트 묶음)

파일 포맷 레벨:
바이트 시퀀스 → 구조화된 데이터
"이 바이트들은 헤더, 저 바이트들은 픽셀 데이터"

코덱 레벨:
압축된 바이트 → 원본 픽셀 복원

애플리케이션 레벨:
픽셀 배열 → 화면의 영상
```

모든 디지털 미디어는 결국 **인코딩**(의미 있는 데이터 → 바이트), **저장/전송**(바이트는 그냥 숫자), **디코딩**(바이트 → 의미 있는 데이터)의 세 단계를 거친다. 영상, 음악, 문서, 프로그램 모두 같은 원리다. 바이트는 같지만 **해석 방법**이 다를 뿐이다.

---

## 6. 멀티파트 업로드로 돌아와서

이제 멀티파트 업로드가 왜 작동하는지 명확하다.

```
파일 = 바이트의 순서가 있는 배열
멀티파트 업로드 = 배열을 청크로 분할
Complete = 청크들을 원래 순서로 연결

video.mp4 [0...N]
    ↓ split
part1 [0...N/3]
part2 [N/3...2N/3]
part3 [2N/3...N]
    ↓ complete
video.mp4 [0...N] ✓
```

컴퓨터 아키텍처 관점에서 보면, 메모리나 디스크나 S3나 결국 "주소 지정 가능한 바이트 배열"이고, 파일은 그 배열의 일부분일 뿐이다. 멀티파트 업로드는 이 근본적인 단순함을 활용한 것이다.

<MultipartVerifier />

</FileProvider>

---

## 7. 프론트엔드에서 S3에 업로드하기: Presigned URL

멀티파트 업로드의 원리를 이해했다면, 실제 웹 애플리케이션에서 어떻게 구현하는지 궁금할 것이다. 프론트엔드에서 S3에 직접 파일을 업로드하려면 **Presigned URL**이라는 개념을 이해해야 한다.

### Presigned URL이 필요한 이유

S3에 접근하려면 AWS Access Key와 Secret Key가 필요하다. 하지만 이 자격증명을 브라우저에 노출하면 심각한 보안 문제가 발생한다.

```javascript
// ❌ 절대 이렇게 하면 안 됨!
const s3 = new S3Client({
  credentials: {
    accessKeyId: 'AKIAIOSFODNN7EXAMPLE',
    secretAccessKey: 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',
  },
});
```

Presigned URL은 **임시로 접근 권한이 부여된 URL**을 생성하여, 자격증명 없이도 S3에 파일을 업로드할 수 있게 한다.

### 작동 방식

백엔드에서 AWS SDK를 사용해 Presigned URL을 생성한다.

```javascript
// 백엔드 (Node.js)
import { S3Client, PutObjectCommand } from '@aws-sdk/client-s3';
import { getSignedUrl } from '@aws-sdk/s3-request-presigner';

const command = new PutObjectCommand({
  Bucket: 'my-bucket',
  Key: 'uploads/photo.jpg',
  ContentType: 'image/jpeg',
});

const presignedUrl = await getSignedUrl(s3Client, command, {
  expiresIn: 3600, // 1시간 동안 유효
});
```

생성된 URL은 다음과 같은 형태다.

```
https://my-bucket.s3.amazonaws.com/uploads/photo.jpg?
X-Amz-Algorithm=AWS4-HMAC-SHA256&
X-Amz-Credential=AKIAIOSFODNN7EXAMPLE%2F...&
X-Amz-Date=20241224T120000Z&
X-Amz-Expires=3600&
X-Amz-Signature=abc123...
```

URL 자체에 **서명**(Signature)이 포함되어 있어, 이 URL을 가진 사람은 지정된 시간 동안만 해당 작업을 수행할 수 있다. 프론트엔드는 이 URL로 S3에 직접 업로드한다.

```javascript
// 프론트엔드
const uploadFile = async (file) => {
  // 1. 백엔드에서 Presigned URL 받기
  const { uploadUrl } = await fetch('/api/get-upload-url', {
    method: 'POST',
    body: JSON.stringify({ filename: file.name, contentType: file.type }),
  }).then((r) => r.json());

  // 2. S3에 직접 업로드
  await fetch(uploadUrl, {
    method: 'PUT',
    body: file,
    headers: { 'Content-Type': file.type },
  });
};
```

<PresignedUrlFlowDemo />

### 멀티파트 업로드와 Presigned URL

대용량 파일의 경우, 각 파트마다 Presigned URL을 생성하여 병렬로 업로드할 수 있다.

```javascript
// 백엔드: 멀티파트 초기화 + 각 파트의 Presigned URL 생성
app.post('/api/init-multipart', async (req, res) => {
  const { filename, partCount } = req.body;

  // 1. 멀티파트 업로드 초기화
  const { UploadId } = await s3Client.send(
    new CreateMultipartUploadCommand({ Bucket: 'my-bucket', Key: filename }),
  );

  // 2. 각 파트의 Presigned URL 생성
  const presignedUrls = await Promise.all(
    Array.from({ length: partCount }, async (_, i) => {
      const command = new UploadPartCommand({
        Bucket: 'my-bucket',
        Key: filename,
        UploadId,
        PartNumber: i + 1,
      });
      return { partNumber: i + 1, url: await getSignedUrl(s3Client, command) };
    }),
  );

  res.json({ uploadId: UploadId, presignedUrls });
});
```

프론트엔드에서는 파일을 분할하고 병렬로 업로드한다.

```javascript
// 프론트엔드: 병렬 업로드
const uploadMultipart = async (file) => {
  const partSize = 5 * 1024 * 1024; // 5MB
  const partCount = Math.ceil(file.size / partSize);

  // 1. Presigned URL들 받기
  const { uploadId, presignedUrls } = await fetch('/api/init-multipart', {
    method: 'POST',
    body: JSON.stringify({ filename: file.name, partCount }),
  }).then((r) => r.json());

  // 2. 병렬 업로드
  const parts = await Promise.all(
    presignedUrls.map(async ({ partNumber, url }) => {
      const start = (partNumber - 1) * partSize;
      const blob = file.slice(start, Math.min(start + partSize, file.size));
      const res = await fetch(url, { method: 'PUT', body: blob });
      return { PartNumber: partNumber, ETag: res.headers.get('ETag') };
    }),
  );

  // 3. Complete 요청
  await fetch('/api/complete-multipart', {
    method: 'POST',
    body: JSON.stringify({ uploadId, filename: file.name, parts }),
  });
};
```

### 왜 Presigned URL을 사용하는가?

파일을 백엔드를 경유해서 업로드할 수도 있다. 하지만 100MB 파일을 업로드한다고 생각해보자.

**백엔드 경유 방식:**

- 사용자 → 백엔드: 100MB 전송
- 백엔드 → S3: 100MB 전송
- 총 트래픽: 200MB, 백엔드 서버 부하 발생

**Presigned URL 방식:**

- 사용자 → 백엔드: URL 요청 (수 KB)
- 사용자 → S3: 100MB 직접 전송
- 총 트래픽: 100MB, 백엔드 부하 없음

파일이 백엔드를 거치지 않으므로 서버 비용과 응답 시간 모두 개선된다.

### 보안

Presigned URL은 다음과 같은 보안 특징을 가진다.

- **시간 제한**: `expiresIn`으로 유효 시간 설정 (최대 7일)
- **작업 제한**: PUT만 가능하도록, 또는 GET만 가능하도록 제한
- **경로 제한**: 지정된 파일 경로에만 접근 가능
- **자격증명 비노출**: URL에 서명만 포함, 실제 Access Key는 포함되지 않음

URL이 유출되더라도 지정된 시간이 지나면 무용지물이 되고, 지정된 작업만 수행할 수 있기 때문에 직접 자격증명을 노출하는 것보다 훨씬 안전하다.
