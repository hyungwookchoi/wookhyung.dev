---
title: 'S3 멀티파트 업로드의 원리: 바이트에서 영상까지'
summary: 대용량 파일을 분할하고 합치는 원리를 바이트 레벨에서 이해하기
date: '2025-12-23'
tags: ['AWS']
draft: false
withClaude: true
---

대용량 파일을 S3에 업로드할 때 멀티파트 업로드를 사용하게 된다. 하지만 "파일을 쪼개서 업로드한다"는 개념이 실제로 어떻게 작동하는지, 그리고 왜 쪼개진 파일들이 다시 합쳐져서 완벽하게 작동할 수 있는지 깊이 이해하려면 파일의 본질부터 이해할 필요가 있다.

---

## S3 멀티파트 업로드란?

S3 멀티파트 업로드는 큰 파일을 여러 개의 작은 파트로 나누어 업로드하는 방식이다. AWS에서는 100MB 이상의 파일에 대해서는 멀티파트 업로드 사용을 권장하고, 5GB 이상의 파일은 반드시 멀티파트 업로드를 사용해야 한다.

### 작동 방식

멀티파트 업로드는 세 단계로 진행된다:

1. **초기화 (Initiate)**: 멀티파트 업로드를 초기화하면 고유한 Upload ID를 받는다
2. **파트 업로드 (Upload Parts)**: 파일을 여러 파트(최소 5MB, 마지막 파트는 예외)로 나누어 각각 업로드한다. 각 파트는 1부터 10,000까지의 번호를 받으며, 업로드 완료 시 ETag를 받는다
3. **완료 (Complete)**: Complete Multipart Upload 요청으로 파트들을 하나의 객체로 결합한다

### 주요 장점

- **복원력**: 네트워크 문제로 업로드가 중단되어도 실패한 파트만 다시 업로드하면 된다
- **성능**: 여러 파트를 병렬로 업로드할 수 있어 대용량 파일의 업로드 속도를 크게 개선할 수 있다
- **유연성**: 파일 크기를 미리 알 필요 없이 스트리밍 방식으로 업로드할 수도 있다

### 실무 주의사항

멀티파트 업로드를 시작했지만 완료하거나 중단하지 않으면 S3에 불완전한 파트들이 계속 저장되어 비용이 발생한다. 수명 주기 정책(Lifecycle Policy)으로 7일 이상 된 불완전한 멀티파트 업로드를 자동으로 삭제하도록 설정하는 것이 권장된다.

<MultipartProcessDemo />

---

## Complete Multipart Upload의 동작 원리

Complete Multipart Upload는 S3에 "이제 모든 파트를 업로드했으니 이것들을 하나의 객체로 합쳐달라"고 요청하는 API 호출이다.

### 요청 구조

Complete 요청의 본문은 XML 형태로, 각 파트의 번호와 ETag를 포함한다:

```xml
<CompleteMultipartUpload>
  <Part>
    <PartNumber>1</PartNumber>
    <ETag>"etag-value-1"</ETag>
  </Part>
  <Part>
    <PartNumber>2</PartNumber>
    <ETag>"etag-value-2"</ETag>
  </Part>
</CompleteMultipartUpload>
```

### 내부 동작 과정

Complete 요청을 받으면 S3는 다음과 같은 과정을 수행한다:

1. **검증**: 제공된 각 ETag가 실제로 업로드된 파트의 ETag와 일치하는지 확인한다
2. **순서 확인**: 파트 번호가 연속적이고 올바른 순서인지 검증한다
3. **결합**: 파트들을 순서대로 연결하여 최종 객체를 생성한다
4. **메타데이터 생성**: 최종 객체의 ETag를 계산한다
5. **정리**: 임시로 저장되어 있던 개별 파트 데이터를 정리한다

### 멀티파트 ETag의 특이점

일반 업로드의 ETag는 파일 전체의 MD5 해시이지만, 멀티파트 업로드로 생성된 객체의 ETag는 다르다:

```
"d41d8cd98f00b204e9800998ecf8427e-3"
```

마지막의 `-3`은 3개의 파트로 업로드되었다는 의미다. 실제 해시값은 각 파트의 MD5를 연결한 후 그것의 MD5를 계산한 것이다:

```
ETag = MD5(MD5(part1) + MD5(part2) + MD5(part3)) + "-" + partCount
```

### 효율적인 설계

S3는 분산 스토리지 시스템이기 때문에, 각 파트는 다른 물리적 위치에 저장될 수 있다. Complete 요청이 들어오면 S3는 실제 데이터를 물리적으로 복사하는 것이 아니라, "이 객체는 파트 1, 2, 3으로 구성되어 있다"는 메타데이터를 생성한다. 이 방식 덕분에 수 GB의 파일도 Complete 요청은 몇 초 안에 완료된다.

<ETagVisualization />

---

## 파일의 본질: 바이트의 연속

여기서 핵심적인 질문이 생긴다. 영상 파일을 예로 들면, 어떻게 쪼개져 있는 각 파트의 영상 파일이 합쳐져 하나의 파일이 될 수 있는 것일까? 이를 이해하려면 파일이 무엇인지부터 알아야 한다.

### 파일은 순서가 있는 바이트의 나열이다

파일은 결국 **순서가 있는 바이트의 나열**이다. 영상 파일도 마찬가지다.

```
video.mp4 = [byte0, byte1, byte2, ..., byteN]
```

예를 들어 100MB 영상 파일이 있다면:

```
총 크기: 104,857,600 bytes
실제 내용: 0x00 0x00 0x00 0x18 0x66 0x74 0x79 0x70 ...
```

이것을 3개 파트로 나누면:

```
Part 1: bytes [0        ~ 34,952,533]  (33.3MB)
Part 2: bytes [34,952,534 ~ 69,905,066]  (33.3MB)
Part 3: bytes [69,905,067 ~ 104,857,599] (33.4MB)
```

### 영상 파일의 구조

MP4 파일은 "박스(box)" 또는 "아톰(atom)"이라는 구조로 이루어져 있다:

```
[ftyp box - 파일 타입]
[moov box - 메타데이터: 코덱, 해상도, 재생시간 등]
  ├─ [mvhd - 전체 정보]
  ├─ [trak - 비디오 트랙]
  └─ [trak - 오디오 트랙]
[mdat box - 실제 비디오/오디오 데이터]
  ├─ 프레임 1 데이터
  ├─ 프레임 2 데이터
  └─ ...
```

하지만 **멀티파트 업로드는 이런 구조를 전혀 신경 쓰지 않는다**. 단순히 바이트 배열을 분할할 뿐이다:

```python
file_bytes = read_entire_file()
part_size = len(file_bytes) // 3

part1 = file_bytes[0:part_size]
part2 = file_bytes[part_size:part_size*2]
part3 = file_bytes[part_size*2:]
```

영상 파일의 헤더 중간을 자르든, 특정 프레임 중간을 자르든 상관없다. 그냥 바이트를 기계적으로 나눈다.

### 다시 합치는 과정

멀티파트 Complete는 단순히 바이트를 순서대로 연결(concatenate)한다:

```python
final_file = part1 + part2 + part3
```

원본 파일:

```
00 00 00 18 66 74 79 70 6D 70 34 32 00 00 00 00 ...
```

Part 1:

```
00 00 00 18 66 74 79 70
```

Part 2:

```
6D 70 34 32 00 00 00 00
```

이것들을 순서대로 붙이면 원본과 정확히 동일한 바이트 시퀀스가 복원된다.

### 실제 검증

터미널에서 직접 확인할 수 있다:

```bash
# 1. 테스트 영상 파일 3개 파트로 분할
split -n 3 video.mp4 part_

# 2. 다시 합치기
cat part_aa part_ab part_ac > reconstructed.mp4

# 3. 원본과 비교
md5sum video.mp4
md5sum reconstructed.mp4
# -> 두 해시값이 완전히 동일!
```

영상 플레이어는 `reconstructed.mp4`를 원본과 똑같이 재생한다. 왜냐하면 바이트 레벨에서 완전히 동일하기 때문이다.

### 파일 포맷의 자기 서술성

이런 단순한 방식이 가능한 이유는 **파일 포맷 자체가 자기 서술적(self-describing)**이기 때문이다:

```
[Header]
- 타입: MP4
- 크기: 100MB
- 비디오 데이터 위치: byte 1,024부터 시작

[Video Data at byte 1,024]
...
```

영상 플레이어는 파일의 처음부터 순차적으로 읽기 시작해서 헤더를 파싱하고, "어디에 무엇이 있는지" 파악한 후 그 위치로 점프(seek)해서 데이터를 읽는다. 멀티파트로 나뉘어 업로드되었든, 한 번에 업로드되었든 **최종적으로 바이트가 올바른 순서로 나열되어 있으면** 문제없다.

### 순서의 중요성

만약 순서가 바뀌면:

```python
wrong_file = part2 + part1 + part3
```

- MP4 헤더가 중간에 있게 되어 플레이어가 파일 타입을 인식하지 못한다
- 바이트 순서가 뒤죽박죽이라 재생이 불가능하다

그래서 Complete 요청에 **반드시 PartNumber를 함께 보내야** 하고, S3는 이 순서대로 정확히 연결한다.

<FileProvider>

<HexDumpExplorer />

---

## 바이트가 영상이 되는 과정

이제 더 근본적인 질문으로 들어가보자. 바이트로 표현된 것들이 어떻게 영상이 될 수 있을까?

### 핵심: 해석이 의미를 만든다

바이트는 그 자체로는 그냥 0~255 사이의 숫자일 뿐이다. 같은 바이트 시퀀스라도 **어떻게 해석하느냐**에 따라 전혀 다른 것이 된다.

```
바이트 시퀀스: 0x48 0x65 0x6C 0x6C 0x6F

해석 1 (ASCII 텍스트): "Hello"
해석 2 (정수): 310,939,249,775
해석 3 (RGB 픽셀): 빨강=72, 초록=101, 파랑=108 ...
```

### 영상의 최소 단위: 픽셀

영상은 수많은 픽셀(점)들의 격자다. 각 픽셀은 색상 정보를 가진다.

```
1920x1080 해상도 = 2,073,600개의 픽셀
각 픽셀 = RGB 3개 값 (빨강, 초록, 파랑)
```

### 픽셀을 바이트로 표현

가장 단순한 경우(비압축):

```
픽셀 (x=0, y=0):
  Red:   255 (0xFF) - 밝은 빨강
  Green: 100 (0x64) - 약간의 초록
  Blue:  50  (0x32) - 조금의 파랑

바이트로 저장: FF 64 32
```

전체 프레임 하나:

```
1920 × 1080 픽셀 × 3 bytes (RGB) = 6,220,800 bytes ≈ 6MB
```

30fps 영상 1초:

```
6MB × 30 프레임 = 180MB/초
```

이것이 **비압축 원본 영상**이다. 실제로는 너무 크기 때문에 압축한다.

### 압축: 코덱의 역할

실제 MP4 파일은 원본 데이터를 압축한 것이다. H.264 압축을 예로 들면:

```
원본 프레임 (6MB):
FF 64 32 FF 64 32 FF 64 32 ... (모든 픽셀)

압축 후 (~200KB):
[I-frame 헤더]
[DCT 계수들]
[움직임 벡터들]
[차분 데이터]
```

압축 원리:

**공간적 중복 제거:**

```
하늘의 파란색 픽셀 1000개가 모두 비슷하다면:
원본: [0x00 0x80 0xFF] × 1000 = 3,000 bytes
압축: "픽셀 (0,0)부터 (31,31)까지: 색상 0x0080FF" = ~20 bytes
```

**시간적 중복 제거:**

```
프레임 1: 자동차가 (100, 200) 위치에 있음
프레임 2: 자동차가 (105, 200) 위치에 있음

원본: 두 프레임 모두 완전히 저장 = 12MB
압축: "프레임 1 저장 + 프레임 2는 '5픽셀 오른쪽 이동'" = ~6.2MB
```

### 실제 바이트 구조

실제 MP4 파일의 처음 몇 바이트:

```
00000000  00 00 00 20 66 74 79 70  69 73 6f 6d 00 00 02 00
00000010  69 73 6f 6d 69 73 6f 32  61 76 63 31 6d 70 34 31
00000020  00 00 08 08 66 72 65 65  00 00 00 00 6d 64 61 74
```

해석:

```
00 00 00 20: 박스 크기 = 32 bytes (빅 엔디안 정수)
66 74 79 70: "ftyp" (ASCII) - 파일 타입 박스
69 73 6f 6d: "isom" - ISO Base Media 포맷
6D 64 61 74: "mdat" - 미디어 데이터 박스 시작
```

### 재생 과정: 바이트에서 영상으로

영상 플레이어가 하는 일:

**1. 파싱 (Parsing)**

```javascript
const bytes = readFile('video.mp4');
const boxes = parseMP4Boxes(bytes);

// 메타데이터 읽기
const videoTrack = boxes.moov.trak[0];
const codec = videoTrack.codec; // "H.264"
const width = videoTrack.width; // 1920
const height = videoTrack.height; // 1080
```

**2. 디코딩 (Decoding)**

```javascript
// 압축된 프레임 데이터 읽기
const compressedFrame = boxes.mdat.frames[0]; // 200KB

// H.264 디코더로 압축 해제
const decodedPixels = h264Decoder.decode(compressedFrame);
// 결과: [255,100,50, 255,100,50, ...] (6MB의 RGB 데이터)
```

**3. 렌더링 (Rendering)**

```javascript
// GPU에 픽셀 데이터 전송
const texture = gpu.createTexture(1920, 1080);
texture.upload(decodedPixels);

// 화면에 그리기
for (let y = 0; y < 1080; y++) {
  for (let x = 0; x < 1920; x++) {
    const pixelIndex = (y * 1920 + x) * 3;
    const r = decodedPixels[pixelIndex];
    const g = decodedPixels[pixelIndex + 1];
    const b = decodedPixels[pixelIndex + 2];

    screen.setPixel(x, y, r, g, b);
  }
}
```

### 구체적 예시: 간단한 이미지

2×2 픽셀 이미지를 만들어보자:

```
[빨강] [초록]
[파랑] [흰색]
```

비압축 BMP 파일로 저장하면:

```
// BMP 헤더 (54 bytes)
42 4D 46 00 00 00 00 00 00 00 36 00 00 00 28 00
00 00 02 00 00 00 02 00 00 00 01 00 18 00 ...

// 픽셀 데이터 (2×2×3 = 12 bytes)
00 00 FF  // 파랑 픽셀 (Blue=255, Green=0, Red=0)
FF FF FF  // 흰색 픽셀 (255, 255, 255)
FF 00 00  // 빨강 픽셀 (0, 0, 255)
00 FF 00  // 초록 픽셀 (0, 255, 0)
```

이미지 뷰어가 이 파일을 열면:

1. 헤더 읽기: "2×2 크기의 24비트 BMP다"
2. 픽셀 데이터 읽기: 바이트들을 3개씩 묶어서 RGB로 해석
3. 모니터에 표시: 각 픽셀 위치에 해당 색상 출력

### 하드웨어 레벨: 바이트에서 빛으로

모니터의 각 픽셀은 실제로 빨강, 초록, 파랑 LED/LCD로 구성되어 있다:

```
바이트 값 255 (0xFF) → 전압 5V → LED 최대 밝기
바이트 값 0 (0x00) → 전압 0V → LED 꺼짐
바이트 값 128 (0x80) → 전압 2.5V → LED 절반 밝기
```

RGB(255, 0, 0) 픽셀:

```
빨강 LED: 최대 밝기 ███
초록 LED: 꺼짐 ░░░
파랑 LED: 꺼짐 ░░░
→ 눈에는 빨간색으로 보임
```

<ByteInterpretationDemo />

---

## 계층적 추상화: 전자에서 영상까지

전체 과정을 정리하면:

```
물리 레벨:
전자의 흐름 → 전압 → LED 밝기

하드웨어 레벨:
0과 1 → 바이트 (8비트 묶음)

파일 포맷 레벨:
바이트 시퀀스 → 구조화된 데이터
"이 바이트들은 헤더, 저 바이트들은 픽셀 데이터"

코덱 레벨:
압축된 바이트 → 원본 픽셀 복원

애플리케이션 레벨:
픽셀 배열 → 화면의 영상
```

모든 디지털 미디어는 결국:

1. **인코딩**: 의미 있는 데이터 → 바이트
2. **저장/전송**: 바이트는 그냥 숫자
3. **디코딩**: 바이트 → 의미 있는 데이터

영상, 음악, 문서, 프로그램 모두 같은 원리다. 바이트는 같지만 **해석 방법**이 다를 뿐이다.

---

## 멀티파트 업로드로 돌아와서

이제 멀티파트 업로드가 왜 작동하는지 명확하다:

```
파일 = 바이트의 순서가 있는 배열
멀티파트 업로드 = 배열을 청크로 분할
Complete = 청크들을 원래 순서로 연결

video.mp4 [0...N]
    ↓ split
part1 [0...N/3]
part2 [N/3...2N/3]
part3 [2N/3...N]
    ↓ complete
video.mp4 [0...N] ✓
```

컴퓨터 아키텍처 관점에서 보면, 메모리나 디스크나 S3나 결국 "주소 지정 가능한 바이트 배열"이고, 파일은 그 배열의 일부분일 뿐이다. 멀티파트 업로드는 이 근본적인 단순함을 활용한 것이다.

<MultipartVerifier />

</FileProvider>
